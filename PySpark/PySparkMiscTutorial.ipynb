{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice Session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('Python-and-Spark-for-Big-Data-master\\Spark_for_Machine_Learning\\Recommender_Systems\\movielens_ratings.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([.8, .2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, userCol='userId', itemCol='movieId', ratingCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model= als.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = als_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----------+\n",
      "|movieId|rating|userId| prediction|\n",
      "+-------+------+------+-----------+\n",
      "|      4|   4.0|    26| -0.3157192|\n",
      "|      6|   1.0|    12|  1.3937502|\n",
      "|      6|   2.0|    22|-0.11653066|\n",
      "|      7|   1.0|    16|  1.6723509|\n",
      "|      0|   1.0|    20|   1.048294|\n",
      "|      2|   2.0|    20|  0.9533929|\n",
      "|      4|   2.0|    20|  -1.589823|\n",
      "|      1|   1.0|     5|  1.9950407|\n",
      "|      5|   1.0|     5|   1.583868|\n",
      "|      4|   1.0|    19|  1.7523725|\n",
      "|      7|   1.0|    15|-0.32536155|\n",
      "|      2|   3.0|     9|  2.9005303|\n",
      "|      3|   1.0|     9|  0.5388041|\n",
      "|      7|   1.0|     8|  1.3429217|\n",
      "|      4|   1.0|    23|  1.2591652|\n",
      "|      3|   1.0|     7|  2.0860949|\n",
      "|      0|   3.0|    10|  0.8959279|\n",
      "|      7|   1.0|    10|   2.914962|\n",
      "|      7|   1.0|    24| -1.9151883|\n",
      "|      4|   1.0|    14| 0.77249444|\n",
      "+-------+------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8479814686253768\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_user = test_data.filter(test_data['userId']==1).select(['movieId', 'userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|userId|\n",
      "+-------+------+\n",
      "|     12|     1|\n",
      "|     16|     1|\n",
      "|     27|     1|\n",
      "|     37|     1|\n",
      "|     40|     1|\n",
      "|     41|     1|\n",
      "|     57|     1|\n",
      "|     62|     1|\n",
      "|     74|     1|\n",
      "|     76|     1|\n",
      "|     77|     1|\n",
      "|     78|     1|\n",
      "|     82|     1|\n",
      "|     91|     1|\n",
      "|     92|     1|\n",
      "|     97|     1|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = als_model.transform(single_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+\n",
      "|movieId|userId| prediction|\n",
      "+-------+------+-----------+\n",
      "|     92|     1|   4.253003|\n",
      "|     74|     1|  3.7711267|\n",
      "|     37|     1|  1.6259863|\n",
      "|     62|     1|  1.5885861|\n",
      "|     91|     1|  1.4808776|\n",
      "|     40|     1|  1.0856148|\n",
      "|     12|     1|  1.0779605|\n",
      "|     16|     1|  0.9525912|\n",
      "|     78|     1|  0.9385193|\n",
      "|     77|     1| 0.92725897|\n",
      "|     57|     1|  0.7126682|\n",
      "|     82|     1|  0.6401354|\n",
      "|     76|     1| 0.32178092|\n",
      "|     41|     1|0.047453385|\n",
      "|     27|     1|-0.14780186|\n",
      "|     97|     1| -0.3149649|\n",
      "+-------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations.orderBy('prediction', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Natural_Language_Processing/smsspamcollection/SMSSpamCollection', inferSchema=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed('_c0', 'class')\n",
    "data = data.withColumnRenamed('_c1', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thatÂ’s th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length', length(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(inputCol='text', outputCol='token_text')\n",
    "stop_remove = StopWordsRemover(inputCol='token_text', outputCol='stop_token')\n",
    "count_vec = CountVectorizer(inputCol='stop_token', outputCol='c_vec')\n",
    "idf = IDF(inputCol='c_vec', outputCol='tf_idf')\n",
    "ham_spam_to_numeric = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_numeric, token, stop_remove, count_vec, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = clean_data.randomSplit([.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detector = nb.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = spam_detector.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,7,8...|[-794.99329284923...|[1.0,5.0601312561...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[-1157.9901064674...|[1.0,1.2313078567...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,15,...|[-659.65634317950...|[1.0,2.2260287482...|       0.0|\n",
      "|  0.0|(13424,[0,1,12,33...|[-450.32106693743...|[1.0,2.3326598184...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,18...|[-1363.7670320217...|[1.0,8.4731730267...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,31...|[-216.41799955703...|[1.0,5.3591285686...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,79...|[-707.02978757946...|[1.0,2.8351413823...|       0.0|\n",
      "|  0.0|(13424,[0,1,15,20...|[-671.96231242256...|[1.0,3.5681269702...|       0.0|\n",
      "|  0.0|(13424,[0,1,21,27...|[-757.66686386345...|[1.0,3.3317183043...|       0.0|\n",
      "|  0.0|(13424,[0,1,23,63...|[-1317.4928019030...|[1.0,5.1091899080...|       0.0|\n",
      "|  0.0|(13424,[0,1,30,11...|[-599.04942045593...|[1.0,1.2944538512...|       0.0|\n",
      "|  0.0|(13424,[0,1,31,43...|[-341.65560431431...|[1.0,1.6096300681...|       0.0|\n",
      "|  0.0|(13424,[0,1,428,6...|[-302.38418362030...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(13424,[0,1,881,1...|[-95.443480707504...|[0.99999998669569...|       0.0|\n",
      "|  0.0|(13424,[0,1,881,1...|[-97.131249783377...|[0.99999998970267...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,4,6...|[-1283.4331928013...|[1.0,9.5136421791...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,8,2...|[-1588.7624300570...|[1.0,2.4447522849...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,2...|[-1412.6411572291...|[1.0,2.4649854432...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,2...|[-567.12141791825...|[1.0,8.3468984832...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,11,...|[-1130.8487437286...|[1.0,7.2406609381...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = acc_eval.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182349264211297"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Streaming Data with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
